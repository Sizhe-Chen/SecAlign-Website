<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="SecAlign: Defending Against Prompt Injection with Preference Optimization">
  <meta name="keywords" content="AI security, prompt injection defense, LLM-integrated applications">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SecAlign: Defending Against Prompt Injection with Preference Optimization</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://sizhe-chen.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Prompt Injection Defenses
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://arxiv.org/pdf/2410.09102">
            ISE (Zoom, Princeton)
          </a>
          <a class="navbar-item" href="https://arxiv.org/pdf/2404.13208">
            Instruction Hierarchy (OpenAI)
          </a>
          <a class="navbar-item" href="https://sizhe-chen.github.io/StruQ-Website">
            StruQ (UC Berkeley)
          </a>
          <a class="navbar-item" href="https://arxiv.org/pdf/2312.17673">
            Jatmo (UC Berkeley, KAIST, PKU)
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SecAlign: Defending Against Prompt Injection with Preference Optimization</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://sizhe-chen.github.io">Sizhe Chen</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://arman-z.github.io">Arman Zharmagambetov</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://smahloujifar.github.io">Saeed Mahloujifar</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://cseweb.ucsd.edu/~kamalika">Kamalika Chaudhuri</a><sup>1</sup>,
            </span>
             <span class="author-block">
              <a href="https://people.eecs.berkeley.edu/~daw">David Wagner</a><sup>2</sup>
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/view/chuanguo">Chuan Guo</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Meta, FAIR</span>
            <span class="author-block"><sup>2</sup>University of California, Berkeley</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2410.05451"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Poster Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1-HFnET2azKniaS4k5dvgVwoRLa4Eg584/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-fw fa-file-alt"></i>
                  </span>
                  <span>Poster</span>
                </a>
              </span>
              <!-- Slides Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1baUbgFMILhPWBeGrm67XXy_H-jO7raRa/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-fw fa-file-archive"></i>
                  </span>
                  <span>Slides</span>
                  </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/facebookresearch/SecAlign"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <br> SecAlign formulates prompt injection defense as preference optimization, reducing the success rate of strong optimization-based attacks to around 0% while preserving model utility.
            </span>
          </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advances in Large Language Models (LLMs) enable exciting LLM-integrated applications, which perform text-based tasks by utilizing their advanced language capabilities. However, as LLMs have improved, so have the attacks against them. <a href="https://www.ibm.com/topics/prompt-injection">Prompt injection attack</a> is listed as the <a href="https://owasp.org/www-project-top-10-for-large-language-model-applications">#1 threat</a> to LLM-integrated applications, where an LLM input contains a trusted prompt (instruction) and an untrusted data (user documents, web retrieval, results from API calls, etc) with potentially injected instructions (Ignore previous instructions and â€¦) to arbitrarily manipulate the LLM.
          </p>
          <p>
            To mitigate this vulnerability, we propose a new defense called
            SecAlign based on the technique of preference optimization. Our
            defense first constructs a preference dataset with prompt-injected
            inputs, secure outputs (ones that respond to the legitimate instruction), and insecure outputs (ones that respond to the injection).
            We then perform preference optimization on this dataset to teach
            the LLM to prefer the secure output over the insecure one. This
            provides the first known method that reduces the success rates of
            various prompt injections to around 0%, even against attacks much
            more sophisticated than ones seen during training. This indicates
            our defense generalizes well against unknown and yet-to-come
            attacks. Also, our defended models are still practical with similar
            utility to the one before our defensive training
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Background -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Background</h2>
        <h3 class="title is-4">LLM-Integrated Applications</h3>
        <div class="content has-text-justified">
          <p>
            Design an instruction (prompt) to serve users by processing their data via an LLM.
          </p>
          <p>
            &#x2022; Prompt: <font color="#00AF50">Trusted</font> (from the developer)
          </p>
          <p>
            &#x2022; LLM: <font color="#00AF50">Trusted</font> (from the developer or an API provider)
          </p>
          <p>
            &#x2022; Data: <font color="#FF0000">Untrusted</font> (from a random user)
          </p>
        </div>
        <div class="content has-text-centered">
          <img src="./static/images/llm_integrated_applications.png" width="100%"/>
        </div>
        <h3 class="title is-4">Prompt Injection Attack</h3>
        <div class="content has-text-justified">
          <p>
            The adversary injects an instruction in data to override the prompted instruction.
          </p>
          <p>
            Listed as the #1 security threat for LLM-integrated applications by OWASP.
          </p>
          <p>
            Example: A university wants to evaluate applicantsâ€™ CV with an LLM.
          </p>
        </div>
        <div class="content has-text-centered">
          <img src="./static/images/prompt_injection_attack.png" width="100%"/>
        </div>
      </div>
    </div>
    <!--/ Background -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Method -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Secure Alignment (SecAlign)</h2>
        <div class="content has-text-justified">
          <p>
            SecAlign constructs a new preference dataset for secure preference optimization.
          </p>
          <p>
            For each sample s in the supervised fine-tuning dataset
          </p>
          <p>
            &#x2022; Sample another random sample sâ€™ for simulating injection
          <p>
            &#x2022; <b>LLM input</b>: prompt-injected s with the instruction in sâ€™
          </p>
          <p>
            &#x2022; <b>Desirable LLM output</b>: the labelled output of s
          </p>
          <p>
            &#x2022; <b>Undesirable LLM output</b>: the labelled output of sâ€™
          </p>
        </div>
        <div class="content has-text-centered">
          <img src="./static/images/teaser.png" width="75%"/>
        </div>
        <div class="content has-text-justified">
          <p>
            SecAlign uses Direct Preference Optimization loss to fine-tune on the secure preference dataset.
          </p>
        </div>
        <div class="content has-text-centered">
          <img src="./static/images/dpo.png" width="75%"/>
        </div>
        <div class="content has-text-justified">
          <p>
            SecAlign trains on simulated injected inputs, labelled with both and desirable responses and undesirable responses, leading to much larger probability gap between outputting them, and thus better robustness against prompt injection attacks.
          </p>
        </div>
        <div class="content has-text-centered">
          <img src="./static/images/motivation.png" width="75%"/>
        </div>
      </div>
    </div>
    <!--/ Method -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Experiments -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Experiments</h2>
        <div class="content has-text-justified">
          <p>
            SecAlign LLMs maintain general-purpose utility (AlpacaEval2 WinRate).
          </p>
          <p>
            SecAlign enjoys 0% attack success rates under optimization-free attacks (Max ASR Opt.-Free). 
          </p>
          <p>
            SecAlign significantly reduces the success rates of optimization-based attacks (Max ASR Opt.-Based) by a factor of >4 from the current SOTA StruQ to <15%.
          </p>
        </div>
        <div class="content has-text-centered">
          <img src="./static/images/main_instruct_intro.png" width="100%"/>
        </div>

        <div class="content has-text-justified">
          <p>
            SecAlign significantly surpasses existing prompting-based and fine-tuning-based defenses (left).
          </p>
        </div>

      <div class="content has-text-justified">
          <p>
            SecAlign LLM is much harder to attack: see a significantly higher attack loss than StruQ (right).
          </p>
      </div>
        <div class="content has-text-centered">
          <img src="./static/images/baseline.png" width="53%"/>
          <img src="./static/images/gcg.png" width="46%"/>
        </div>
    </div>
    <!--/ Experiments -->
  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <div class="columns is-centered has-text-centered"><h2 class="title">BibTeX</h2></div>
    <div class="columns is-centered">
      <div class="column is-full-width">
    <pre><code>@article{chen2024aligning,
  title={SecAlign: Defending Against Prompt Injection with Preference Optimization},
  author={Chen, Sizhe and Zharmagambetov, Arman and Mahloujifar, Saeed and Chaudhuri, Kamalika and Wagner, David and Guo, Chuan},
  journal={arXiv preprint arXiv:2410.05451},
  year={2024}
}</code></pre>
    </div>
    </div>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/Sizhe-Chen/SecAlign-Website">source code</a>,
            we just ask that you link back to this page in the footer.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
