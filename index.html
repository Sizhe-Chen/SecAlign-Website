<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="SecAlign: Defending Against Prompt Injection with Preference Optimization">
  <meta name="keywords" content="AI security, prompt injection defense, LLM-integrated applications">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>SecAlign: Defending Against Prompt Injection with Preference Optimization</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://sizhe-chen.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Prompt Injection Defenses
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://arxiv.org/pdf/2410.09102">
            ISE (Zoom, Princeton)
          </a>
          <a class="navbar-item" href="https://arxiv.org/pdf/2503.24370">
            Thinking Intervene (Princeton)
          </a>
          <a class="navbar-item" href="https://arxiv.org/pdf/2404.13208">
            Instruction Hierarchy (OpenAI)
          </a>
          <a class="navbar-item" href="https://sizhe-chen.github.io/StruQ-Website">
            StruQ (UC Berkeley)
          </a>
          <a class="navbar-item" href="https://arxiv.org/pdf/2312.17673">
            Jatmo (UC Berkeley)
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">SecAlign: Defending Against Prompt Injection with Preference Optimization</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://sizhe-chen.github.io">Sizhe Chen</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://arman-z.github.io">Arman Zharmagambetov</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://smahloujifar.github.io">Saeed Mahloujifar</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://cseweb.ucsd.edu/~kamalika">Kamalika Chaudhuri</a><sup>1</sup>,
            </span>
             <span class="author-block">
              <a href="https://people.eecs.berkeley.edu/~daw">David Wagner</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/view/chuanguo">Chuan Guo</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Meta, FAIR</span>
            <span class="author-block"><sup>2</sup>University of California, Berkeley</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2410.05451"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>CCS'25</span>
                </a>
              </span>
              <!-- Poster Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1-HFnET2azKniaS4k5dvgVwoRLa4Eg584/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-fw fa-file-alt"></i>
                  </span>
                  <span>Poster</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://bair.berkeley.edu/blog/2025/04/11/prompt-injection-defense"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-blog"></i>
                  </span>
                  <span>Blog</span>
                </a>
              </span>
              <!-- Lecture Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1g0BVB5HCMjJU4IBGWfdUVope4gr5V_cL/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-fw fa-file-archive"></i>
                  </span>
                  <span>Lecture</span>
                  </a>
              </span>
              <!-- Slides Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1baUbgFMILhPWBeGrm67XXy_H-jO7raRa/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-fw fa-file-archive"></i>
                  </span>
                  <span>Slides</span>
                  </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/facebookresearch/SecAlign"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <br> SecAlign formulates prompt injection defense as preference optimization, reducing the success rates by a factor of >4 from the previous SOTA StruQ without non-trivial utility loss.
            </span>
          </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advances in Large Language Models (LLMs) enable exciting LLM-integrated applications, which perform text-based tasks by utilizing their advanced language capabilities. However, as LLMs have improved, so have the attacks against them. <a href="https://www.ibm.com/topics/prompt-injection">Prompt injection attack</a> is listed as the <a href="https://owasp.org/www-project-top-10-for-large-language-model-applications">#1 threat</a> to LLM-integrated applications, where an LLM input contains a trusted prompt (instruction) and an untrusted data (user documents, web retrieval, results from API calls, etc) with potentially injected instructions (Ignore previous instructions and â€¦) to arbitrarily manipulate the LLM.
          </p>
          <p>
            To mitigate this vulnerability, we propose a new defense called
            SecAlign based on the technique of preference optimization. Our
            defense first constructs a preference dataset with prompt-injected
            inputs, secure outputs (ones that respond to the legitimate instruction), and insecure outputs (ones that respond to the injection).
            We then perform preference optimization on this dataset to teach
            the LLM to prefer the secure output over the insecure one. This
            provides the first known method that reduces the success rates of
            various prompt injections to less than 10%, even against attacks much
            more sophisticated than ones seen during training. This indicates
            our defense generalizes well against unknown and yet-to-come
            attacks. Also, our defended models are still practical with similar
            utility to the one before our defensive training.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Background -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Background</h2>
        <h3 class="title is-4">LLM-Integrated Applications</h3>
        <div class="content has-text-justified">
          <p>
            Design an instruction (prompt) to serve users by processing their data via an LLM.
          </p>
          <p>
            &#x2022; Prompt: <font color="#00AF50">Trusted</font> (from the developer)
          </p>
          <p>
            &#x2022; LLM: <font color="#00AF50">Trusted</font> (from the developer or an API provider)
          </p>
          <p>
            &#x2022; Data: <font color="#FF0000">Untrusted</font> (from a random user)
          </p>
        </div>
        <div class="content has-text-centered">
          <img src="./static/images/llm_integrated_applications.png" width="100%"/>
        </div>
        <h3 class="title is-4">Prompt Injection Attack</h3>
        <div class="content has-text-justified">
          <p>
            The adversary injects an instruction in data to override the prompted instruction.
          </p>
          <p>
            Listed as the #1 security threat for LLM-integrated applications by OWASP.
          </p>
          <p>
            Example: A university wants to evaluate applicantsâ€™ CV with an LLM.
          </p>
        </div>
        <div class="content has-text-centered">
          <img src="./static/images/prompt_injection_attack.png" width="100%"/>
        </div>
      </div>
    </div>
    <!--/ Background -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Method -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Secure Alignment (SecAlign)</h2>
        <div class="content has-text-justified">
          <p>
            (1) Get an SFT model by downloading a public instruct model (recommended) or SFTing a base model.
          </p>
          <p>
            (2) Save the modelâ€™s delimiters.
          </p>
          <p>
            (3) Find a public instruction tuning dataset.
          </p>
          <p>
            (4) Construct the preference dataset. For each sample s in the instruction tuning dataset
          </p>
          <p>
            &#x2022; Sample another random sample sâ€™ for simulating injection
          <p>
            &#x2022; <b>LLM input</b>: prompt-injected s with the instruction in sâ€™
          </p>
          <p>
            &#x2022; <b>Desirable LLM output</b>: the labelled output of s
          </p>
          <p>
            &#x2022; <b>Undesirable LLM output</b>: the labelled output of sâ€™
          </p>
        </div>
        <div class="content has-text-centered">
          <img src="./static/images/teaser.png" width="75%"/>
        </div>
        <div class="content has-text-justified">
          <p>
            (5) Preference-optimize the SFT model on the preference dataset. SecAlign uses Direct Preference Optimization loss.
          </p>
        </div>
        <div class="content has-text-centered">
          <img src="./static/images/dpo.png" width="75%"/>
        </div>
        <div class="content has-text-justified">
          <p>
            SecAlign trains on simulated injected inputs, labelled with both desirable responses and undesirable responses, leading to much larger probability gap between outputting them, and thus better robustness against prompt injection attacks.
          </p>
        </div>
        <div class="content has-text-centered">
          <img src="./static/images/motivation.png" width="75%"/>
        </div>
      </div>
    </div>
    <!--/ Method -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Experiments -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Experiments</h2>
        <div class="content has-text-justified">
          <p>
            SecAlign Llama3-8B-Instruct maintains general-purpose utility (AlpacaEval2 WinRate).
          </p>
          <p>
            SecAlign Llama3-8B-Instruct enjoys 8% attack success rates under the strongest tested optimization-based prompt injections. 
          </p>
           <p>
            SecAlign significantly surpasses existing prompting-based and fine-tuning-based defenses.
          </p>
        </div>
        <div class="content has-text-centered">
          <img src="./static/images/main_instruct_intro.png" width="75%"/>
        </div>
    </div>
    <!--/ Experiments -->
  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <div class="columns is-centered has-text-centered"><h2 class="title">BibTeX</h2></div>
    <div class="columns is-centered">
      <div class="column is-full-width">
    <pre><code>@inproceedings{chen2025secalign,
  title={SecAlign: Defending Against Prompt Injection with Preference Optimization},
  author={Chen, Sizhe and Zharmagambetov, Arman and Mahloujifar, Saeed and Chaudhuri, Kamalika and Wagner, David and Guo, Chuan},
  booktitle={The ACM Conference on Computer and Communications Security (CCS)},
  year={2025}
}</code></pre>
    </div>
    </div>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/Sizhe-Chen/SecAlign-Website">source code</a>,
            we just ask that you link back to this page in the footer.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
